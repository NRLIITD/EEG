{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5e4d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import mne\n",
    "import copy\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn.functional as F\n",
    "import time as t\n",
    "import sklearn.metrics as metrics\n",
    "from scipy import signal\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ba90c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "4ec2bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest = \"/home/cbme/phd/bmz198612/EEG/EEG_Subject/S22/Rest\"\n",
    "easy = \"/home/cbme/phd/bmz198612/EEG/EEG_Subject/S22/Easy\"\n",
    "med = \"/home/cbme/phd/bmz198612/EEG/EEG_Subject/S22/Med\"\n",
    "hard = \"/home/cbme/phd/bmz198612/EEG/EEG_Subject/S22/Hard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "8e3e8cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cbme/phd/bmz198612/EEG/EEG_Subject/S22/Easy/easy12nbStr2S22.mat\n",
      "easy12nbStr2S22\n"
     ]
    }
   ],
   "source": [
    "for temp in glob(easy+\"/*.mat\"):\n",
    "    data=scipy.io.loadmat(temp)\n",
    "    print(temp)\n",
    "    temp=temp.replace(easy,\"\")\n",
    "    temp=temp.replace('.mat','')\n",
    "    temp=temp.replace('/','')\n",
    "    print(temp)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "95a472ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'easy12nbStr2S22'])"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "1bbdacd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 12000)"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[temp]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "a767b5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Measurement date</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Experimenter</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "        <th>Participant</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Digitized points</th>\n",
       "        \n",
       "        <td>3 points</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Good channels</th>\n",
       "        <td>63 misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Bad channels</th>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>EOG channels</th>\n",
       "        <td>Not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ECG channels</th>\n",
       "        <td>Not available</td>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Sampling frequency</th>\n",
       "        <td>200.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Highpass</th>\n",
       "        <td>0.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Lowpass</th>\n",
       "        <td>100.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "</table>"
      ],
      "text/plain": [
       "<Info | 8 non-empty values\n",
       " bads: []\n",
       " ch_names: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, ...\n",
       " chs: 63 misc\n",
       " custom_ref_applied: False\n",
       " dig: 3 items (3 Cardinal)\n",
       " highpass: 0.0 Hz\n",
       " lowpass: 100.0 Hz\n",
       " meas_date: unspecified\n",
       " nchan: 63\n",
       " projs: []\n",
       " sfreq: 200.0 Hz\n",
       ">"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_chan = 63\n",
    "sampling_freq = 200\n",
    "ch_types = ['eeg']*63\n",
    "info = mne.create_info(num_chan,sampling_freq)\n",
    "info.set_montage('standard_1020')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "4de0aca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=63, n_times=12000\n",
      "    Range : 0 ... 11999 =      0.000 ...    59.995 secs\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "data = mne.io.RawArray(data,info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "4072238c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "39 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "epochs = mne.make_fixed_length_epochs(data,duration=3,overlap=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "b2833913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from preloaded Raw for 39 events and 600 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(39, 63, 600)"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs.get_data().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "f9735bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "rest_data = []\n",
    "stress_data = []\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "7b5b08aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertmat2mne(data):\n",
    "    ch_types=['eeg']*63\n",
    "    ch_names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62']    \n",
    "    info = mne.create_info(ch_names,ch_types=ch_types,sfreq=200)\n",
    "    data = mne.io.RawArray(data,info)\n",
    "    data.set_eeg_reference()\n",
    "    epochs=mne.make_fixed_length_epochs(data,duration=3,overlap=2.4)\n",
    "    return epochs.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "c98172e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from glob import glob\n",
    "import scipy.io\n",
    "for temp in glob(rest+\"/*.mat\"):\n",
    "    data=scipy.io.loadmat(temp)\n",
    "    index=temp.replace(rest,\"\")\n",
    "    index=index.replace('.mat','')\n",
    "    index=index.replace('/','')\n",
    "    data=data[index]\n",
    "    data = convertmat2mne(data)\n",
    "    rest_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "64c783ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from glob import glob\n",
    "import scipy.io\n",
    "for temp in glob(easy+\"/*.mat\"):\n",
    "    data=scipy.io.loadmat(temp)\n",
    "    index=temp.replace(easy,\"\")\n",
    "    index=index.replace('.mat','')\n",
    "    index=index.replace('/','')\n",
    "    data=data[index]\n",
    "    data = convertmat2mne(data)\n",
    "    stress_data.append(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "a4e82af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from glob import glob\n",
    "import scipy.io\n",
    "for temp in glob(med+\"/*.mat\"):\n",
    "    data=scipy.io.loadmat(temp)\n",
    "    index=temp.replace(med,\"\")\n",
    "    index=index.replace('.mat','')\n",
    "    index=index.replace('/','')\n",
    "    data=data[index]\n",
    "    data = convertmat2mne(data)\n",
    "    stress_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "584aeeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from glob import glob\n",
    "import scipy.io\n",
    "for temp in glob(hard+\"/*.mat\"):\n",
    "    data=scipy.io.loadmat(temp)\n",
    "    index=temp.replace(hard,\"\")\n",
    "    index=index.replace('.mat','')\n",
    "    index=index.replace('/','')\n",
    "    data=data[index]\n",
    "    data = convertmat2mne(data)\n",
    "    stress_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "b11351fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_epochs_labels = [len(i)*[0] for i in rest_data]\n",
    "stress_epochs_labels = [len(i)*[1] for i in stress_data]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "4b67170d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 15\n"
     ]
    }
   ],
   "source": [
    "data_list = rest_data+stress_data\n",
    "label_list = rest_epochs_labels+stress_epochs_labels\n",
    "print(len(data_list),len(label_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "5f295d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_list = [[i]*len(j) for i ,j in enumerate(data_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "908cb732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groups_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "eec956dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 600, 63) (1440,)\n"
     ]
    }
   ],
   "source": [
    "data_array = np.vstack(data_list)\n",
    "label_array = np.hstack(label_list)\n",
    "group_array = np.hstack(groups_list)\n",
    "data_array = np.moveaxis(data_array,1,2)\n",
    "\n",
    "print(data_array.shape,label_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "b3404e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold,GroupKFold,LeaveOneGroupOut,GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#https://stackoverflow.com/questions/50125844/how-to-standard-scale-a-3d-matrix\n",
    "class StandardScaler3D(BaseEstimator,TransformerMixin):\n",
    "    #batch, sequence, channels\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        self.scaler.fit(X.reshape(-1, X.shape[2]))\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        return self.scaler.transform(X.reshape( -1,X.shape[2])).reshape(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "9b7fd7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = data_array.reshape((data_array.shape[0],1,data_array.shape[1],data_array.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "a9da0325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 600, 63)"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_array[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "12f59df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1440, 1, 600, 63)"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "fcdd5ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(f'Reset trainable parameters of layer = {layer}')\n",
    "            layer.reset_parameters()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "687bf280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=1,out_channels=32,kernel_size=(1,25),stride=1)\n",
    "        self.conv2=nn.Conv2d(in_channels=32,out_channels=32,kernel_size=(63,1),stride=1)\n",
    "        self.BN = nn.BatchNorm2d(32)\n",
    "        self.GL = nn.GELU()\n",
    "        self.AP = nn.AvgPool2d((1,50),(1,25))\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.lstm = nn.LSTM(32,64,2,batch_first=True,dropout=0.4)\n",
    "        self.linear = nn.Linear(64,2)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=32,nhead=8,dim_feedforward=128,batch_first=True,norm_first=True),num_layers=1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(1184, 4)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.conv2(x)\n",
    "        x=self.BN(x)\n",
    "        x=self.GL(x)\n",
    "        x=self.AP(x)\n",
    "        x=self.dropout(x)\n",
    "        #print(x.shape)\n",
    "        b,e,h,w = x.shape\n",
    "        x=x.view(b,h*w,e)\n",
    "        x,_=self.lstm(x)\n",
    "        x=self.linear(x[:,-1,:])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "62a144fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input=torch.randn(16,1,63,600)\n",
    "conformer=Model()\n",
    "\n",
    "test_model = Model()\n",
    "out=conformer(input)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "1ab090bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conformer = conformer.to(device)\n",
    "test_model = test_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "9cc83cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "precision=[]\n",
    "accuracy=[]\n",
    "recall=[]\n",
    "f1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "3ef00d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader,train_data, device,optimizer,criterion):\n",
    "    print('Training')\n",
    "    model.train()\n",
    "    counter = 0\n",
    "    train_running_loss = 0.0\n",
    "    loss_values=[]\n",
    "    running_corrects = 0.0\n",
    "    for batch_index, (x, y) in enumerate(dataloader):\n",
    "        counter += 1\n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        # apply sigmoid activation to get all the outputs between 0 and 1\n",
    "        y = y.to(torch.int64)\n",
    "        loss = criterion(outputs, y)\n",
    "        train_running_loss += loss.item()\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        # update optimizer parameters\n",
    "        optimizer.step()\n",
    "        train_loss = train_running_loss / len(dataloader)\n",
    "        loss_values.append(train_loss)\n",
    "    return train_loss\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "022d6292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader,val_data, device,optimizer,criterion):\n",
    "    print('validating')\n",
    "    model.eval()\n",
    "    counter = 0\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "         for batch_index, (x, y) in enumerate(dataloader):\n",
    "            counter += 1\n",
    "            x=x.to(device)\n",
    "            y=y.to(device)\n",
    "            outputs = model(x)\n",
    "            y = y.to(torch.int64)\n",
    "            loss = criterion(outputs, y)\n",
    "            val_running_loss += loss.item()\n",
    "            val_loss = val_running_loss / len(dataloader)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "59a1e020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader,test_data, device,optimizer,criterion):\n",
    "    y_target = []\n",
    "    y_pred = []\n",
    "    print('Testing')\n",
    "    checkpoint = torch.load('/home/cbme/phd/bmz198612/EEG/Conformer/conformer1.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "         for batch_index, (x, y) in enumerate(dataloader):\n",
    "            x=x.to(device)\n",
    "            y=y.to(device)\n",
    "            outputs = model(x)\n",
    "            y = y.to(torch.int64)\n",
    "            _,prediction=torch.max(outputs.data,1)\n",
    "            y_target.extend(y.cpu().data.numpy().tolist())\n",
    "            y_pred.extend(prediction.cpu().data.numpy().tolist())\n",
    "            \n",
    "            \n",
    "    p = metrics.precision_score(y_target,y_pred,average=\"macro\")\n",
    "    precision.append(p)\n",
    "    a = metrics.accuracy_score(y_target,y_pred)\n",
    "    accuracy.append(a)\n",
    "    r = metrics.recall_score(y_target,y_pred,average=\"macro\")\n",
    "    recall.append(r)\n",
    "    f = metrics.f1_score(y_target,y_pred,average=\"macro\")\n",
    "    f1.append(f)\n",
    "    print(p)\n",
    "    print(a)\n",
    "    print(r)\n",
    "    print(f)\n",
    "         \n",
    "         \n",
    "     \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "6df214e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([854, 1, 63, 600])\n",
      "torch.Size([106, 1, 63, 600])\n",
      "torch.Size([480, 1, 63, 600])\n",
      "Epoch 1 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.5219\n",
      "Val Loss: 0.4726\n",
      "Epoch 2 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4712\n",
      "Val Loss: 0.4703\n",
      "Epoch 3 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4603\n",
      "Val Loss: 0.4740\n",
      "Epoch 4 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4636\n",
      "Val Loss: 0.4756\n",
      "Epoch 5 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4600\n",
      "Val Loss: 0.4673\n",
      "Epoch 6 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4593\n",
      "Val Loss: 0.4710\n",
      "Epoch 7 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4529\n",
      "Val Loss: 0.4686\n",
      "Epoch 8 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4529\n",
      "Val Loss: 0.4889\n",
      "Epoch 9 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4427\n",
      "Val Loss: 0.4803\n",
      "Epoch 10 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4143\n",
      "Val Loss: 0.4124\n",
      "Epoch 11 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3747\n",
      "Val Loss: 0.3753\n",
      "Epoch 12 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3694\n",
      "Val Loss: 0.3674\n",
      "Epoch 13 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3463\n",
      "Val Loss: 0.3653\n",
      "Epoch 14 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3516\n",
      "Val Loss: 0.3721\n",
      "Epoch 15 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3446\n",
      "Val Loss: 0.3651\n",
      "Epoch 16 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3539\n",
      "Val Loss: 0.3560\n",
      "Epoch 17 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3278\n",
      "Val Loss: 0.3405\n",
      "Epoch 18 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3375\n",
      "Val Loss: 0.3436\n",
      "Epoch 19 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3164\n",
      "Val Loss: 0.3129\n",
      "Epoch 20 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3051\n",
      "Val Loss: 0.3082\n",
      "Epoch 21 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3074\n",
      "Val Loss: 0.3302\n",
      "Epoch 22 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2979\n",
      "Val Loss: 0.3069\n",
      "Epoch 23 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2920\n",
      "Val Loss: 0.3005\n",
      "Epoch 24 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2942\n",
      "Val Loss: 0.3247\n",
      "Epoch 25 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2835\n",
      "Val Loss: 0.2990\n",
      "Epoch 26 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2763\n",
      "Val Loss: 0.2705\n",
      "Epoch 27 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2774\n",
      "Val Loss: 0.2610\n",
      "Epoch 28 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2616\n",
      "Val Loss: 0.2645\n",
      "Epoch 29 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2595\n",
      "Val Loss: 0.2589\n",
      "Epoch 30 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2610\n",
      "Val Loss: 0.2452\n",
      "Epoch 31 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2699\n",
      "Val Loss: 0.2542\n",
      "Epoch 32 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2592\n",
      "Val Loss: 0.2358\n",
      "Epoch 33 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2460\n",
      "Val Loss: 0.2349\n",
      "Epoch 34 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2422\n",
      "Val Loss: 0.2341\n",
      "Epoch 35 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2565\n",
      "Val Loss: 0.2233\n",
      "Epoch 36 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2438\n",
      "Val Loss: 0.2291\n",
      "Epoch 37 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2281\n",
      "Val Loss: 0.2366\n",
      "Epoch 38 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2262\n",
      "Val Loss: 0.2234\n",
      "Epoch 39 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2332\n",
      "Val Loss: 0.2179\n",
      "Epoch 40 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2298\n",
      "Val Loss: 0.2091\n",
      "Epoch 41 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2271\n",
      "Val Loss: 0.2428\n",
      "Epoch 42 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2219\n",
      "Val Loss: 0.2192\n",
      "Epoch 43 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2181\n",
      "Val Loss: 0.2467\n",
      "Epoch 44 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2245\n",
      "Val Loss: 0.2012\n",
      "Epoch 45 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2038\n",
      "Val Loss: 0.1898\n",
      "Epoch 46 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1986\n",
      "Val Loss: 0.1940\n",
      "Epoch 47 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1981\n",
      "Val Loss: 0.1961\n",
      "Epoch 48 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2106\n",
      "Val Loss: 0.1996\n",
      "Epoch 49 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1989\n",
      "Val Loss: 0.1824\n",
      "Epoch 50 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1978\n",
      "Val Loss: 0.1887\n",
      "20.087488651275635\n",
      "Testing\n",
      "0.5\n",
      "0.73125\n",
      "0.5\n",
      "0.4931196227866961\n",
      "Reset trainable parameters of layer = NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=32, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=32, bias=True)\n",
      "Reset trainable parameters of layer = LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=1184, out_features=4, bias=True)\n",
      "Reset trainable parameters of layer = Conv2d(1, 32, kernel_size=(1, 25), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(32, 32, kernel_size=(63, 1), stride=(1, 1))\n",
      "Reset trainable parameters of layer = BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = LSTM(32, 64, num_layers=2, batch_first=True, dropout=0.4)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=2, bias=True)\n",
      "torch.Size([854, 1, 63, 600])\n",
      "torch.Size([106, 1, 63, 600])\n",
      "torch.Size([480, 1, 63, 600])\n",
      "Epoch 1 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4911\n",
      "Val Loss: 0.4388\n",
      "Epoch 2 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4283\n",
      "Val Loss: 0.4265\n",
      "Epoch 3 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4137\n",
      "Val Loss: 0.4209\n",
      "Epoch 4 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4146\n",
      "Val Loss: 0.4198\n",
      "Epoch 5 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4033\n",
      "Val Loss: 0.4070\n",
      "Epoch 6 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4027\n",
      "Val Loss: 0.4019\n",
      "Epoch 7 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3794\n",
      "Val Loss: 0.3748\n",
      "Epoch 8 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3740\n",
      "Val Loss: 0.3777\n",
      "Epoch 9 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3676\n",
      "Val Loss: 0.3741\n",
      "Epoch 10 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3210\n",
      "Val Loss: 0.2495\n",
      "Epoch 11 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2890\n",
      "Val Loss: 0.2354\n",
      "Epoch 12 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2731\n",
      "Val Loss: 0.2247\n",
      "Epoch 13 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2905\n",
      "Val Loss: 0.2369\n",
      "Epoch 14 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2189\n",
      "Val Loss: 0.2151\n",
      "Epoch 15 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2754\n",
      "Val Loss: 0.2192\n",
      "Epoch 16 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2198\n",
      "Val Loss: 0.1563\n",
      "Epoch 17 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2259\n",
      "Val Loss: 0.1447\n",
      "Epoch 18 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2039\n",
      "Val Loss: 0.1314\n",
      "Epoch 19 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2006\n",
      "Val Loss: 0.1374\n",
      "Epoch 20 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1954\n",
      "Val Loss: 0.1284\n",
      "Epoch 21 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1926\n",
      "Val Loss: 0.1164\n",
      "Epoch 22 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1882\n",
      "Val Loss: 0.1109\n",
      "Epoch 23 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.2013\n",
      "Val Loss: 0.1389\n",
      "Epoch 24 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1818\n",
      "Val Loss: 0.1286\n",
      "Epoch 25 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1867\n",
      "Val Loss: 0.1172\n",
      "Epoch 26 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1702\n",
      "Val Loss: 0.1157\n",
      "Epoch 27 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1675\n",
      "Val Loss: 0.1085\n",
      "Epoch 28 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1570\n",
      "Val Loss: 0.1014\n",
      "Epoch 29 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1441\n",
      "Val Loss: 0.0985\n",
      "Epoch 30 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1571\n",
      "Val Loss: 0.0968\n",
      "Epoch 31 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1479\n",
      "Val Loss: 0.0944\n",
      "Epoch 32 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1540\n",
      "Val Loss: 0.0950\n",
      "Epoch 33 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1458\n",
      "Val Loss: 0.0934\n",
      "Epoch 34 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1477\n",
      "Val Loss: 0.0952\n",
      "Epoch 35 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1501\n",
      "Val Loss: 0.0924\n",
      "Epoch 36 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1325\n",
      "Val Loss: 0.0913\n",
      "Epoch 37 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1401\n",
      "Val Loss: 0.0897\n",
      "Epoch 38 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1432\n",
      "Val Loss: 0.0925\n",
      "Epoch 39 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1330\n",
      "Val Loss: 0.0927\n",
      "Epoch 40 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1327\n",
      "Val Loss: 0.0899\n",
      "Epoch 41 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1405\n",
      "Val Loss: 0.0899\n",
      "Epoch 42 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1365\n",
      "Val Loss: 0.0904\n",
      "Epoch 43 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1332\n",
      "Val Loss: 0.0898\n",
      "Epoch 44 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1308\n",
      "Val Loss: 0.0895\n",
      "Epoch 45 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1312\n",
      "Val Loss: 0.0892\n",
      "Epoch 46 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1359\n",
      "Val Loss: 0.0891\n",
      "Epoch 47 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1367\n",
      "Val Loss: 0.0888\n",
      "Epoch 48 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1281\n",
      "Val Loss: 0.0889\n",
      "Epoch 49 of 50\n",
      "Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating\n",
      "Train Loss: 0.1445\n",
      "Val Loss: 0.0889\n",
      "Epoch 50 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.1404\n",
      "Val Loss: 0.0889\n",
      "20.055817127227783\n",
      "Testing\n",
      "0.5459110473457676\n",
      "0.68125\n",
      "0.5546875\n",
      "0.5474460651100882\n",
      "Reset trainable parameters of layer = NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=32, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=32, bias=True)\n",
      "Reset trainable parameters of layer = LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=1184, out_features=4, bias=True)\n",
      "Reset trainable parameters of layer = Conv2d(1, 32, kernel_size=(1, 25), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(32, 32, kernel_size=(63, 1), stride=(1, 1))\n",
      "Reset trainable parameters of layer = BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = LSTM(32, 64, num_layers=2, batch_first=True, dropout=0.4)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=2, bias=True)\n",
      "torch.Size([854, 1, 63, 600])\n",
      "torch.Size([106, 1, 63, 600])\n",
      "torch.Size([480, 1, 63, 600])\n",
      "Epoch 1 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.5208\n",
      "Val Loss: 0.4556\n",
      "Epoch 2 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4589\n",
      "Val Loss: 0.4529\n",
      "Epoch 3 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4470\n",
      "Val Loss: 0.4373\n",
      "Epoch 4 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4485\n",
      "Val Loss: 0.4560\n",
      "Epoch 5 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4426\n",
      "Val Loss: 0.4486\n",
      "Epoch 6 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4417\n",
      "Val Loss: 0.4459\n",
      "Epoch 7 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4337\n",
      "Val Loss: 0.4486\n",
      "Epoch 8 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4292\n",
      "Val Loss: 0.4528\n",
      "Epoch 9 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.4147\n",
      "Val Loss: 0.4223\n",
      "Epoch 10 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3980\n",
      "Val Loss: 0.4076\n",
      "Epoch 11 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3949\n",
      "Val Loss: 0.3961\n",
      "Epoch 12 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3914\n",
      "Val Loss: 0.3962\n",
      "Epoch 13 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3878\n",
      "Val Loss: 0.3907\n",
      "Epoch 14 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3906\n",
      "Val Loss: 0.3960\n",
      "Epoch 15 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3794\n",
      "Val Loss: 0.3905\n",
      "Epoch 16 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3862\n",
      "Val Loss: 0.3949\n",
      "Epoch 17 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3771\n",
      "Val Loss: 0.3829\n",
      "Epoch 18 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3675\n",
      "Val Loss: 0.3553\n",
      "Epoch 19 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3566\n",
      "Val Loss: 0.3489\n",
      "Epoch 20 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3621\n",
      "Val Loss: 0.3473\n",
      "Epoch 21 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3437\n",
      "Val Loss: 0.3327\n",
      "Epoch 22 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3406\n",
      "Val Loss: 0.3340\n",
      "Epoch 23 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3465\n",
      "Val Loss: 0.3371\n",
      "Epoch 24 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3570\n",
      "Val Loss: 0.3528\n",
      "Epoch 25 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3383\n",
      "Val Loss: 0.3299\n",
      "Epoch 26 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3484\n",
      "Val Loss: 0.3427\n",
      "Epoch 27 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3444\n",
      "Val Loss: 0.3382\n",
      "Epoch 28 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3250\n",
      "Val Loss: 0.3206\n",
      "Epoch 29 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3325\n",
      "Val Loss: 0.3267\n",
      "Epoch 30 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3449\n",
      "Val Loss: 0.3274\n",
      "Epoch 31 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3397\n",
      "Val Loss: 0.3384\n",
      "Epoch 32 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3239\n",
      "Val Loss: 0.3210\n",
      "Epoch 33 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3251\n",
      "Val Loss: 0.3126\n",
      "Epoch 34 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3258\n",
      "Val Loss: 0.3110\n",
      "Epoch 35 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3188\n",
      "Val Loss: 0.3096\n",
      "Epoch 36 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3097\n",
      "Val Loss: 0.3072\n",
      "Epoch 37 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3111\n",
      "Val Loss: 0.3046\n",
      "Epoch 38 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3172\n",
      "Val Loss: 0.3024\n",
      "Epoch 39 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3068\n",
      "Val Loss: 0.2989\n",
      "Epoch 40 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3105\n",
      "Val Loss: 0.2957\n",
      "Epoch 41 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3140\n",
      "Val Loss: 0.2935\n",
      "Epoch 42 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3063\n",
      "Val Loss: 0.2925\n",
      "Epoch 43 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3031\n",
      "Val Loss: 0.2926\n",
      "Epoch 44 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3013\n",
      "Val Loss: 0.2923\n",
      "Epoch 45 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3026\n",
      "Val Loss: 0.2932\n",
      "Epoch 46 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3106\n",
      "Val Loss: 0.2919\n",
      "Epoch 47 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3149\n",
      "Val Loss: 0.2936\n",
      "Epoch 48 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3010\n",
      "Val Loss: 0.2939\n",
      "Epoch 49 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3072\n",
      "Val Loss: 0.2933\n",
      "Epoch 50 of 50\n",
      "Training\n",
      "validating\n",
      "Train Loss: 0.3005\n",
      "Val Loss: 0.2921\n",
      "20.078201055526733\n",
      "Testing\n",
      "0.7977540106951873\n",
      "0.8520833333333333\n",
      "0.6888020833333334\n",
      "0.721019327270197\n",
      "Reset trainable parameters of layer = NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=32, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=32, bias=True)\n",
      "Reset trainable parameters of layer = LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=1184, out_features=4, bias=True)\n",
      "Reset trainable parameters of layer = Conv2d(1, 32, kernel_size=(1, 25), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(32, 32, kernel_size=(63, 1), stride=(1, 1))\n",
      "Reset trainable parameters of layer = BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = LSTM(32, 64, num_layers=2, batch_first=True, dropout=0.4)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "gkf=StratifiedGroupKFold(n_splits=3)\n",
    "for train_index, test_index in gkf.split(data_array, label_array, groups=group_array):\n",
    "    train_features,train_labels,train_group=data_array[train_index],label_array[train_index],group_array[train_index]\n",
    "    \n",
    "    test_features,test_labels=data_array[test_index],label_array[test_index]\n",
    "    \n",
    "    scaler=StandardScaler3D()\n",
    "    train_features=scaler.fit_transform(train_features)\n",
    "    test_features=scaler.transform(test_features)\n",
    "    train_features=np.moveaxis(train_features,2,3)\n",
    "    test_features=np.moveaxis(test_features,2,3)\n",
    "    train_features = torch.Tensor(train_features)\n",
    "    test_features = torch.Tensor(test_features)\n",
    "    train_labels = torch.Tensor(train_labels)\n",
    "    test_labels = torch.Tensor(test_labels)\n",
    "   \n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.11, random_state=42)\n",
    "    print(X_train.shape)\n",
    "    val_data = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "    print(X_val.shape)\n",
    "    train_data = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "    print(test_features.shape)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size = 16, shuffle = False)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size = 32, shuffle = False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size = 32, shuffle = False)\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    epochs = 50\n",
    "    start = t.time()\n",
    "    optimizer = optim.Adam(conformer.parameters(), lr = 0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=0.1, patience=3)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "        valid_min_loss = np.Inf\n",
    "        train_epoch_loss = train(conformer,train_loader,train_data,device,optimizer,criterion)\n",
    "        valid_epoch_loss = validate(conformer,val_loader,val_data,device,optimizer,criterion)\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        valid_loss.append(valid_epoch_loss)\n",
    "        scheduler.step(valid_epoch_loss)\n",
    "        print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "        print(f'Val Loss: {valid_epoch_loss:.4f}')\n",
    "        if(valid_epoch_loss <= valid_min_loss):\n",
    "            valid_min_loss = valid_epoch_loss\n",
    "            torch.save({\n",
    "                'epoch': epochs,\n",
    "                'model_state_dict': conformer.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,\n",
    "                }, '/home/cbme/phd/bmz198612/EEG/Conformer/conformer1.pth')\n",
    "    print(t.time()-start)\n",
    "    test(test_model,test_loader,test_data,device,optimizer,criterion)\n",
    "    conformer.apply(reset_weights)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "9b63572c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.614555019346985 0.7548611111111111 0.5811631944444445 0.5871950050556605\n"
     ]
    }
   ],
   "source": [
    "print(sum(precision)/3,sum(accuracy)/3,sum(recall)/3,sum(f1)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "329ba182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nkf = KFold(n_splits=5)\\nbatch_size=64\\nfor i, (train_index, test_index) in enumerate(kf.split(eeg_data)):\\n        ### Dividing data into folds\\n        print(f\"Fold {i}:\")\\n        x_train_fold = eeg_data[train_index]\\n        x_test_fold = eeg_data[test_index]\\n        y_train_fold = labels[train_index]\\n        y_test_fold = labels[test_index]\\n        X_train, X_val, y_train, y_val = train_test_split(x_train_fold, y_train_fold, test_size=0.1, random_state=42)\\n        val_data = torch.utils.data.TensorDataset(X_val, y_val)\\n        train_data = torch.utils.data.TensorDataset(X_train, y_train)\\n        test_data = torch.utils.data.TensorDataset(x_test_fold, y_test_fold)\\n        train_loader = torch.utils.data.DataLoader(train_data, batch_size = 16, shuffle = False)\\n        val_loader = torch.utils.data.DataLoader(val_data, batch_size = 32, shuffle = False)\\n        test_loader = torch.utils.data.DataLoader(test_data, batch_size = 32, shuffle = False)\\n        \\n        train_loss = []\\n        valid_loss = []\\n        epochs = 50\\n        start = t.time()\\n        optimizer = optim.Adam(conformer.parameters(), lr = 0.001)\\n        criterion = nn.CrossEntropyLoss()\\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=0.1, patience=3)\\n        for epoch in range(epochs):\\n            print(f\"Epoch {epoch+1} of {epochs}\")\\n            valid_min_loss = np.Inf\\n            train_epoch_loss = train(conformer,train_loader,train_data,device,optimizer,criterion)\\n            valid_epoch_loss = validate(conformer,val_loader,val_data,device,optimizer,criterion)\\n            train_loss.append(train_epoch_loss)\\n            valid_loss.append(valid_epoch_loss)\\n            scheduler.step(valid_epoch_loss)\\n            print(f\"Train Loss: {train_epoch_loss:.4f}\")\\n            print(f\\'Val Loss: {valid_epoch_loss:.4f}\\')\\n            if(valid_epoch_loss <= valid_min_loss):\\n                valid_min_loss = valid_epoch_loss\\n                torch.save({\\n                    \\'epoch\\': epochs,\\n                    \\'model_state_dict\\': conformer.state_dict(),\\n                    \\'optimizer_state_dict\\': optimizer.state_dict(),\\n                    \\'loss\\': criterion,\\n                    }, \\'C:/Users/Anoop Godiyal/Desktop/Path/conformer.pth\\')\\n        print(t.time()-start)\\n        \\n        test(test_model,test_loader,test_data,device,optimizer,criterion)\\n        conformer.apply(reset_weights)\\n'"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "kf = KFold(n_splits=5)\n",
    "batch_size=64\n",
    "for i, (train_index, test_index) in enumerate(kf.split(eeg_data)):\n",
    "        ### Dividing data into folds\n",
    "        print(f\"Fold {i}:\")\n",
    "        x_train_fold = eeg_data[train_index]\n",
    "        x_test_fold = eeg_data[test_index]\n",
    "        y_train_fold = labels[train_index]\n",
    "        y_test_fold = labels[test_index]\n",
    "        X_train, X_val, y_train, y_val = train_test_split(x_train_fold, y_train_fold, test_size=0.1, random_state=42)\n",
    "        val_data = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "        train_data = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "        test_data = torch.utils.data.TensorDataset(x_test_fold, y_test_fold)\n",
    "        train_loader = torch.utils.data.DataLoader(train_data, batch_size = 16, shuffle = False)\n",
    "        val_loader = torch.utils.data.DataLoader(val_data, batch_size = 32, shuffle = False)\n",
    "        test_loader = torch.utils.data.DataLoader(test_data, batch_size = 32, shuffle = False)\n",
    "        \n",
    "        train_loss = []\n",
    "        valid_loss = []\n",
    "        epochs = 50\n",
    "        start = t.time()\n",
    "        optimizer = optim.Adam(conformer.parameters(), lr = 0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=0.1, patience=3)\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "            valid_min_loss = np.Inf\n",
    "            train_epoch_loss = train(conformer,train_loader,train_data,device,optimizer,criterion)\n",
    "            valid_epoch_loss = validate(conformer,val_loader,val_data,device,optimizer,criterion)\n",
    "            train_loss.append(train_epoch_loss)\n",
    "            valid_loss.append(valid_epoch_loss)\n",
    "            scheduler.step(valid_epoch_loss)\n",
    "            print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "            print(f'Val Loss: {valid_epoch_loss:.4f}')\n",
    "            if(valid_epoch_loss <= valid_min_loss):\n",
    "                valid_min_loss = valid_epoch_loss\n",
    "                torch.save({\n",
    "                    'epoch': epochs,\n",
    "                    'model_state_dict': conformer.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': criterion,\n",
    "                    }, 'C:/Users/Anoop Godiyal/Desktop/Path/conformer.pth')\n",
    "        print(t.time()-start)\n",
    "        \n",
    "        test(test_model,test_loader,test_data,device,optimizer,criterion)\n",
    "        conformer.apply(reset_weights)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c41456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fe62ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bbd4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dfc53e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83cfb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082de2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ac3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dc8c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0345f97a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edffac26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55f4557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
